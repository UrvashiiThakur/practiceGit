{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9qb4j4KqEMZ/5OMRY1Bc7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/26Mar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTFw4ZxPZ-XC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
        "\n",
        "**Simple Linear Regression**:\n",
        "- **Definition**: Models the relationship between two variables by fitting a linear equation to the observed data. One independent variable is used to predict the dependent variable.\n",
        "- **Example**: Predicting a person's height (dependent variable) based on their age (independent variable).\n",
        "\n",
        "  \\[\n",
        "  \\text{Height} = \\beta_0 + \\beta_1 \\times \\text{Age}\n",
        "  \\]\n",
        "\n",
        "**Multiple Linear Regression**:\n",
        "- **Definition**: Models the relationship between a dependent variable and two or more independent variables by fitting a linear equation to the observed data.\n",
        "- **Example**: Predicting a person's height (dependent variable) based on their age, weight, and gender (independent variables).\n",
        "\n",
        "  \\[\n",
        "  \\text{Height} = \\beta_0 + \\beta_1 \\times \\text{Age} + \\beta_2 \\times \\text{Weight} + \\beta_3 \\times \\text{Gender}\n",
        "  \\]\n",
        "\n",
        "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
        "\n",
        "**Assumptions of Linear Regression**:\n",
        "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
        "2. **Independence**: Observations are independent of each other.\n",
        "3. **Homoscedasticity**: Constant variance of errors.\n",
        "4. **Normality**: The residuals (errors) are normally distributed.\n",
        "5. **No Multicollinearity**: Independent variables are not highly correlated.\n",
        "\n",
        "**Checking Assumptions**:\n",
        "1. **Linearity**: Scatter plots of independent variables against the dependent variable.\n",
        "2. **Independence**: Durbin-Watson test.\n",
        "3. **Homoscedasticity**: Plotting residuals versus fitted values.\n",
        "4. **Normality**: Q-Q plot or Shapiro-Wilk test.\n",
        "5. **No Multicollinearity**: Variance Inflation Factor (VIF) or correlation matrix.\n",
        "\n",
        "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
        "\n",
        "**Interpretation**:\n",
        "- **Intercept (\\(\\beta_0\\))**: The expected value of the dependent variable when all independent variables are zero.\n",
        "- **Slope (\\(\\beta_1\\))**: The change in the dependent variable for a one-unit change in the independent variable.\n",
        "\n",
        "**Example**:\n",
        "- **Scenario**: Predicting house prices based on square footage.\n",
        "  \\[\n",
        "  \\text{Price} = 50000 + 200 \\times \\text{Square Footage}\n",
        "  \\]\n",
        "  - **Intercept (\\(\\beta_0 = 50000\\))**: The base price of a house, assuming zero square footage (though not practical, it serves as the baseline).\n",
        "  - **Slope (\\(\\beta_1 = 200\\))**: For each additional square foot, the price of the house increases by $200.\n",
        "\n",
        "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "\n",
        "**Gradient Descent**:\n",
        "- **Definition**: An optimization algorithm used to minimize the cost function by iteratively moving towards the minimum value.\n",
        "- **Usage in Machine Learning**:\n",
        "  - **Initialization**: Start with random initial values for parameters.\n",
        "  - **Iteration**: Update parameters by moving in the direction of the negative gradient of the cost function.\n",
        "  - **Learning Rate**: Determines the size of the steps taken towards the minimum.\n",
        "  - **Convergence**: The algorithm stops when changes in the cost function are below a certain threshold.\n",
        "\n",
        "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "\n",
        "**Multiple Linear Regression Model**:\n",
        "- **Definition**: An extension of simple linear regression that models the relationship between a dependent variable and multiple independent variables.\n",
        "- **Equation**:\n",
        "  \\[\n",
        "  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon\n",
        "  \\]\n",
        "  - \\(Y\\): Dependent variable.\n",
        "  - \\(\\beta_0\\): Intercept.\n",
        "  - \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\): Slopes of the independent variables \\(X_1, X_2, \\ldots, X_p\\).\n",
        "  - \\(\\epsilon\\): Error term.\n",
        "\n",
        "**Difference from Simple Linear Regression**:\n",
        "- **Simple Linear Regression**: One independent variable.\n",
        "- **Multiple Linear Regression**: Two or more independent variables.\n",
        "\n",
        "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
        "\n",
        "**Multicollinearity**:\n",
        "- **Definition**: Occurs when two or more independent variables in a regression model are highly correlated, making it difficult to determine the individual effect of each variable.\n",
        "- **Detection**:\n",
        "  - **Variance Inflation Factor (VIF)**: VIF values greater than 10 indicate high multicollinearity.\n",
        "  - **Correlation Matrix**: High correlation coefficients between independent variables.\n",
        "- **Addressing Multicollinearity**:\n",
        "  - **Remove Variables**: Drop one of the correlated variables.\n",
        "  - **Combine Variables**: Combine correlated variables into a single feature.\n",
        "  - **Principal Component Analysis (PCA)**: Reduce the dimensionality of the data.\n",
        "\n",
        "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "\n",
        "**Polynomial Regression Model**:\n",
        "- **Definition**: A form of regression analysis where the relationship between the independent variable \\(X\\) and the dependent variable \\(Y\\) is modeled as an nth degree polynomial.\n",
        "- **Equation**:\n",
        "  \\[\n",
        "  Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\cdots + \\beta_n X^n + \\epsilon\n",
        "  \\]\n",
        "\n",
        "**Difference from Linear Regression**:\n",
        "- **Linear Regression**: Assumes a linear relationship between the independent and dependent variables.\n",
        "- **Polynomial Regression**: Models a non-linear relationship by adding polynomial terms (e.g., \\(X^2, X^3\\)) to the regression equation.\n",
        "\n",
        "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
        "\n",
        "**Advantages of Polynomial Regression**:\n",
        "- **Flexibility**: Can model non-linear relationships.\n",
        "- **Better Fit**: Can provide a better fit for complex datasets.\n",
        "\n",
        "**Disadvantages of Polynomial Regression**:\n",
        "- **Overfitting**: Higher-degree polynomials can fit the noise in the data, leading to overfitting.\n",
        "- **Complexity**: More complex than linear regression, harder to interpret.\n",
        "- **Extrapolation Issues**: Predictions outside the range of the data can be unreliable.\n",
        "\n",
        "**Situations to Prefer Polynomial Regression**:\n",
        "- When the relationship between the independent and dependent variables is non-linear.\n",
        "- When a linear model provides a poor fit and adding polynomial terms improves the performance.\n",
        "\n",
        "For instance, modeling the growth of certain biological processes or physical phenomena where the rate of change is not constant might benefit from polynomial regression."
      ],
      "metadata": {
        "id": "7-DfsSuDaFla"
      }
    }
  ]
}