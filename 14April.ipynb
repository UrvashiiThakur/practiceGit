{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVEG67pg8qjVmpkm+RZYX1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/14April.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wol9ZuEB5vcs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Random Forest Regressor?\n",
        "\n",
        "Random Forest Regressor is an ensemble learning method for regression tasks. It operates by constructing a multitude of decision trees during training and outputting the average prediction of the individual trees for regression tasks. This method helps to improve the predictive performance and control overfitting compared to a single decision tree.\n",
        "\n",
        "### Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
        "\n",
        "Random Forest Regressor reduces the risk of overfitting through:\n",
        "1. **Averaging Predictions**: By averaging the results of multiple decision trees, it smooths out the predictions and reduces variance.\n",
        "2. **Bootstrap Aggregating (Bagging)**: Each tree is trained on a different bootstrap sample of the training data, which introduces variability.\n",
        "3. **Random Feature Selection**: At each split in the tree, a random subset of features is considered, which decorrelates the trees and helps in reducing overfitting.\n",
        "\n",
        "### Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
        "\n",
        "In Random Forest Regressor, the predictions from multiple decision trees are aggregated by averaging:\n",
        "- **Regression Tasks**: The final prediction is the mean of all the predictions from individual trees.\n",
        "\n",
        "### Q4. What are the hyperparameters of Random Forest Regressor?\n",
        "\n",
        "Some key hyperparameters of Random Forest Regressor include:\n",
        "- **n_estimators**: Number of trees in the forest.\n",
        "- **max_depth**: Maximum depth of each tree.\n",
        "- **min_samples_split**: Minimum number of samples required to split an internal node.\n",
        "- **min_samples_leaf**: Minimum number of samples required to be at a leaf node.\n",
        "- **max_features**: Number of features to consider when looking for the best split.\n",
        "- **bootstrap**: Whether bootstrap samples are used when building trees.\n",
        "- **oob_score**: Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
        "\n",
        "### Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
        "\n",
        "**Random Forest Regressor**:\n",
        "- **Ensemble Method**: Combines predictions from multiple decision trees.\n",
        "- **Reduced Overfitting**: Less prone to overfitting due to averaging.\n",
        "- **Better Generalization**: Generally performs better on unseen data.\n",
        "\n",
        "**Decision Tree Regressor**:\n",
        "- **Single Model**: Relies on a single decision tree.\n",
        "- **High Variance**: More prone to overfitting if the tree is deep.\n",
        "- **Simple to Interpret**: Easier to understand and visualize.\n",
        "\n",
        "### Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
        "\n",
        "**Advantages**:\n",
        "- **Reduced Overfitting**: Through averaging multiple trees.\n",
        "- **Robustness**: Handles missing values and maintains accuracy for a large proportion of data.\n",
        "- **Flexibility**: Can handle both categorical and numerical data.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Complexity**: Less interpretable compared to a single decision tree.\n",
        "- **Computationally Intensive**: Requires more computation power and memory.\n",
        "- **Slower Predictions**: Due to the ensemble nature, predictions can be slower.\n",
        "\n",
        "### Q7. What is the output of Random Forest Regressor?\n",
        "\n",
        "The output of Random Forest Regressor is a continuous numerical value, which is the average of the predictions made by the individual decision trees in the forest.\n",
        "\n",
        "### Q8. Can Random Forest Regressor be used for classification tasks?\n",
        "\n",
        "No, Random Forest Regressor is specifically designed for regression tasks. However, a similar method called **Random Forest Classifier** is used for classification tasks. In Random Forest Classifier, the output is the mode of the classes predicted by individual trees, rather than an average of numerical values.\n",
        "\n",
        "**References**:\n",
        "- [Scikit-learn: Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
        "- [Ensemble Methods: Random Forests](https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/)\n",
        "- [Understanding Random Forests](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)"
      ],
      "metadata": {
        "id": "EWO69V9_5-oL"
      }
    }
  ]
}