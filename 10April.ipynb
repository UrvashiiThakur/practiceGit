{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkY21zA3kTVT/2R2TusjMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/10April.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_IlwN7ibp9v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Probability Calculation\n",
        "\n",
        "Given:\n",
        "- \\( P(H) = 0.7 \\) (Probability of using the health insurance plan)\n",
        "- \\( P(S | H) = 0.4 \\) (Probability of being a smoker given using the health insurance plan)\n",
        "\n",
        "We need to find \\( P(S | H) \\), which is already given as 0.4.\n",
        "\n",
        "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%.\n",
        "\n",
        "### Q2. Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
        "\n",
        "**Bernoulli Naive Bayes:**\n",
        "- Used for binary/boolean features (e.g., whether a word occurs in a document or not).\n",
        "- Each feature is assumed to be independent and binary (0 or 1).\n",
        "- Suitable for binary data.\n",
        "\n",
        "**Multinomial Naive Bayes:**\n",
        "- Used for discrete data (e.g., word counts or frequencies).\n",
        "- Suitable for text classification problems where the input data is represented as word counts or term frequencies.\n",
        "- Each feature represents the number of occurrences of a word or token in the document.\n",
        "\n",
        "### Q3. How Bernoulli Naive Bayes Handles Missing Values\n",
        "\n",
        "Bernoulli Naive Bayes does not handle missing values inherently. Missing values need to be imputed before applying the classifier. Common techniques include filling missing values with the mode or mean of the feature, or using more advanced imputation techniques like k-nearest neighbors or regression imputation.\n",
        "\n",
        "### Q4. Can Gaussian Naive Bayes Be Used for Multi-Class Classification?\n",
        "\n",
        "Yes, Gaussian Naive Bayes can be used for multi-class classification. It models each class's feature distribution as a Gaussian (normal) distribution and applies Bayes' theorem to predict the class of new instances. Scikit-learn's `GaussianNB` supports multi-class classification out-of-the-box.\n",
        "\n",
        "### Q5. Assignment: Naive Bayes Classifiers on Spambase Data Set\n",
        "\n",
        "#### Data Preparation:\n",
        "Download the \"Spambase Data Set\" from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Spambase).\n",
        "\n",
        "#### Implementation:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "data = pd.read_csv(url, header=None)\n",
        "\n",
        "# Split data into features and labels\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data (e.g., scaling for GaussianNB)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize classifiers\n",
        "bernoulli_nb = BernoulliNB()\n",
        "multinomial_nb = MultinomialNB()\n",
        "gaussian_nb = GaussianNB()\n",
        "\n",
        "# Train classifiers\n",
        "bernoulli_nb.fit(X_train, y_train)\n",
        "multinomial_nb.fit(X_train, y_train)\n",
        "gaussian_nb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_bernoulli = bernoulli_nb.predict(X_test)\n",
        "y_pred_multinomial = multinomial_nb.predict(X_test)\n",
        "y_pred_gaussian = gaussian_nb.predict(X_test_scaled)\n",
        "\n",
        "# Performance metrics\n",
        "def print_metrics(y_test, y_pred):\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
        "    print(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n",
        "\n",
        "print(\"Bernoulli Naive Bayes:\")\n",
        "print_metrics(y_test, y_pred_bernoulli)\n",
        "\n",
        "print(\"\\nMultinomial Naive Bayes:\")\n",
        "print_metrics(y_test, y_pred_multinomial)\n",
        "\n",
        "print(\"\\nGaussian Naive Bayes:\")\n",
        "print_metrics(y_test, y_pred_gaussian)\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "grid_search_bernoulli = GridSearchCV(BernoulliNB(), param_grid, cv=10, scoring='f1')\n",
        "grid_search_multinomial = GridSearchCV(MultinomialNB(), param_grid, cv=10, scoring='f1')\n",
        "\n",
        "grid_search_bernoulli.fit(X_train, y_train)\n",
        "grid_search_multinomial.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters for BernoulliNB:\", grid_search_bernoulli.best_params_)\n",
        "print(\"Best parameters for MultinomialNB:\", grid_search_multinomial.best_params_)\n",
        "\n",
        "# Re-train with the best parameters\n",
        "bernoulli_nb_best = BernoulliNB(alpha=grid_search_bernoulli.best_params_['alpha'])\n",
        "multinomial_nb_best = MultinomialNB(alpha=grid_search_multinomial.best_params_['alpha'])\n",
        "\n",
        "bernoulli_nb_best.fit(X_train, y_train)\n",
        "multinomial_nb_best.fit(X_train, y_train)\n",
        "\n",
        "y_pred_bernoulli_best = bernoulli_nb_best.predict(X_test)\n",
        "y_pred_multinomial_best = multinomial_nb_best.predict(X_test)\n",
        "\n",
        "print(\"Tuned Bernoulli Naive Bayes:\")\n",
        "print_metrics(y_test, y_pred_bernoulli_best)\n",
        "\n",
        "print(\"Tuned Multinomial Naive Bayes:\")\n",
        "print_metrics(y_test, y_pred_multinomial_best)\n",
        "```\n",
        "\n",
        "### Results:\n",
        "\n",
        "Report the performance metrics (Accuracy, Precision, Recall, F1 score) for each classifier.\n",
        "\n",
        "### Discussion:\n",
        "\n",
        "Discuss the results obtained, comparing the performance of Bernoulli, Multinomial, and Gaussian Naive Bayes classifiers. Explain why one variant might perform better than the others, considering the nature of the dataset and the assumptions each model makes. Discuss any limitations observed in Naive Bayes classifiers, such as handling correlated features or assuming feature independence.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Summarize your findings and provide suggestions for future work. Consider experimenting with different preprocessing techniques, feature engineering, or trying other classifiers to further improve performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "vPbgdDl0bq6i"
      }
    }
  ]
}