{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7r5eISWsBHPZxavHezFrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/16_mar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3bxoc9obK1S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
        "\n",
        "**Overfitting**:\n",
        "- **Definition**: Overfitting occurs when a model learns the training data too well, capturing noise and details that do not generalize to new data.\n",
        "- **Consequences**:\n",
        "  - Poor performance on unseen data (test set).\n",
        "  - High variance.\n",
        "- **Mitigation**:\n",
        "  - Use more training data.\n",
        "  - Simplify the model.\n",
        "  - Apply regularization techniques like L1 or L2 regularization.\n",
        "  - Use cross-validation to tune model parameters.\n",
        "  - Prune decision trees if using them.\n",
        "  - Use dropout in neural networks.\n",
        "\n",
        "**Underfitting**:\n",
        "- **Definition**: Underfitting occurs when a model is too simple to capture the underlying patterns in the data.\n",
        "- **Consequences**:\n",
        "  - Poor performance on both training and test sets.\n",
        "  - High bias.\n",
        "- **Mitigation**:\n",
        "  - Increase the complexity of the model (e.g., add more features or layers in neural networks).\n",
        "  - Reduce regularization.\n",
        "  - Provide more relevant features.\n",
        "\n",
        "### Q2: How can we reduce overfitting? Explain in brief.\n",
        "\n",
        "- **Use more training data**: Larger datasets help models generalize better.\n",
        "- **Simplify the model**: Reduce the complexity of the model by decreasing the number of parameters.\n",
        "- **Regularization**: Apply techniques like L1 (Lasso) or L2 (Ridge) regularization to penalize large coefficients.\n",
        "- **Cross-validation**: Use techniques like k-fold cross-validation to ensure the model performs well on different subsets of data.\n",
        "- **Pruning**: In decision trees, prune branches that have little importance.\n",
        "- **Dropout**: In neural networks, use dropout layers to randomly drop units during training, which helps prevent the model from overfitting to the training data.\n",
        "\n",
        "### Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "\n",
        "**Underfitting**:\n",
        "- **Definition**: Underfitting occurs when a model is too simplistic and fails to capture the underlying structure of the data.\n",
        "- **Scenarios**:\n",
        "  - **Insufficient model complexity**: Using a linear model to fit non-linear data.\n",
        "  - **Too much regularization**: Over-penalizing the model parameters can lead to underfitting.\n",
        "  - **Not enough training**: In neural networks, too few epochs can result in underfitting.\n",
        "  - **Poor feature selection**: Using irrelevant or too few features can cause the model to miss important patterns.\n",
        "\n",
        "### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
        "\n",
        "**Bias-Variance Tradeoff**:\n",
        "- **Bias**: Error due to overly simplistic assumptions in the learning algorithm. High bias leads to systematic errors.\n",
        "- **Variance**: Error due to excessive sensitivity to small fluctuations in the training set. High variance leads to overfitting.\n",
        "- **Tradeoff**: There is a tradeoff between bias and variance. As model complexity increases, bias decreases but variance increases, and vice versa. The goal is to find a balance where both bias and variance are minimized, achieving low total error.\n",
        "- **Effect on Performance**:\n",
        "  - **High Bias**: Model underfits the data, leading to poor training and test performance.\n",
        "  - **High Variance**: Model overfits the training data, leading to good training performance but poor test performance.\n",
        "\n",
        "### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
        "\n",
        "**Detecting Overfitting**:\n",
        "- **Performance gap**: Large difference between training and validation/test performance.\n",
        "- **High variance**: Model performs well on training data but poorly on unseen data.\n",
        "\n",
        "**Detecting Underfitting**:\n",
        "- **Poor performance**: Both training and validation/test performance are low.\n",
        "- **High bias**: Model fails to capture underlying patterns in the training data.\n",
        "\n",
        "**Methods**:\n",
        "- **Learning curves**: Plot training and validation error over epochs. Divergence indicates overfitting, while high errors indicate underfitting.\n",
        "- **Cross-validation**: Ensures model performs consistently across different subsets of the data.\n",
        "- **Residual plots**: In regression, plot residuals (differences between actual and predicted values) to identify patterns indicative of underfitting or overfitting.\n",
        "\n",
        "### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
        "\n",
        "**Bias**:\n",
        "- **Definition**: Error due to simplistic assumptions in the model.\n",
        "- **Example**: Linear regression on a non-linear dataset.\n",
        "- **Performance**: Consistent error across training and test sets (underfitting).\n",
        "\n",
        "**Variance**:\n",
        "- **Definition**: Error due to the model's sensitivity to small fluctuations in the training data.\n",
        "- **Example**: A deep decision tree without pruning.\n",
        "- **Performance**: Low training error but high test error (overfitting).\n",
        "\n",
        "**High Bias Model**:\n",
        "- Simple models like linear regression on complex data.\n",
        "- Poor training and test performance.\n",
        "\n",
        "**High Variance Model**:\n",
        "- Complex models like unpruned decision trees or high-degree polynomials.\n",
        "- Good training performance but poor test performance.\n",
        "\n",
        "### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
        "\n",
        "**Regularization**:\n",
        "- **Definition**: Technique used to prevent overfitting by adding a penalty term to the loss function to constrain the model complexity.\n",
        "\n",
        "**Techniques**:\n",
        "- **L1 Regularization (Lasso)**:\n",
        "  - Adds the absolute value of the coefficients as a penalty term.\n",
        "  - Encourages sparsity, setting some coefficients to zero.\n",
        "- **L2 Regularization (Ridge)**:\n",
        "  - Adds the square of the coefficients as a penalty term.\n",
        "  - Penalizes large coefficients more strongly, leading to more evenly distributed weights.\n",
        "- **Elastic Net**:\n",
        "  - Combines L1 and L2 regularization.\n",
        "  - Balances between reducing complexity and maintaining relevant features.\n",
        "- **Dropout**:\n",
        "  - In neural networks, randomly drops units (along with their connections) during training.\n",
        "  - Reduces dependency on specific neurons, preventing overfitting.\n",
        "- **Early Stopping**:\n",
        "  - Monitors model performance on a validation set and stops training when performance degrades.\n",
        "  - Prevents overtraining and overfitting.\n",
        "\n",
        "Regularization helps by adding constraints that limit the model's ability to learn overly complex patterns, ensuring it generalizes better to new data."
      ],
      "metadata": {
        "id": "KlZeJblDbLsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ammmocFTbTJg"
      }
    }
  ]
}