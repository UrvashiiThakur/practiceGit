{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdQxyGxcFsG0YVrbdpT9NW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/17April.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8cijZ18gMDm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Gradient Boosting Regression?\n",
        "Gradient Boosting Regression is an ensemble machine learning technique that combines multiple weak learners, typically decision trees, to form a strong predictive model for regression tasks. The technique builds the model in a stage-wise fashion, where each subsequent model corrects the errors of the previous ones by fitting to the negative gradient of the loss function with respect to the predictions.\n",
        "\n",
        "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy\n",
        "Here is a basic implementation of a gradient boosting regression algorithm from scratch:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def _fit_tree(self, X, y):\n",
        "        from sklearn.tree import DecisionTreeRegressor\n",
        "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "        tree.fit(X, y)\n",
        "        return tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.models = []\n",
        "        # Initialize with mean prediction\n",
        "        y_pred = np.full(y.shape, np.mean(y))\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            tree = self._fit_tree(X, residuals)\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "            self.models.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for tree in self.models:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Simple example\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.datasets import make_regression\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "    # Create a simple regression dataset\n",
        "    X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    # Train the model\n",
        "    gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "    gbr.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = gbr.predict(X_test)\n",
        "    print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\n",
        "    print(f\"R-squared: {r2_score(y_test, y_pred)}\")\n",
        "```\n",
        "\n",
        "### Q3. Experiment with different hyperparameters\n",
        "To optimize the performance, we can use Grid Search to find the best hyperparameters.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "gbr = GradientBoostingRegressor()\n",
        "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best parameters found: {best_params}\")\n",
        "\n",
        "# Train with the best parameters\n",
        "gbr_optimized = GradientBoostingRegressor(**best_params)\n",
        "gbr_optimized.fit(X_train, y_train)\n",
        "y_pred_optimized = gbr_optimized.predict(X_test)\n",
        "\n",
        "print(f\"Optimized Mean Squared Error: {mean_squared_error(y_test, y_pred_optimized)}\")\n",
        "print(f\"Optimized R-squared: {r2_score(y_test, y_pred_optimized)}\")\n",
        "```\n",
        "\n",
        "### Q4. What is a weak learner in Gradient Boosting?\n",
        "A weak learner in Gradient Boosting is typically a simple model that performs slightly better than random guessing. In the context of Gradient Boosting, decision stumps (trees with a single split) or shallow decision trees are often used as weak learners. The idea is to combine many such weak learners to form a strong predictive model.\n",
        "\n",
        "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "The intuition behind Gradient Boosting is to sequentially add models to an ensemble in a way that each new model corrects the errors made by the previous models. This is done by fitting each new model to the negative gradient of the loss function with respect to the current ensemble's predictions, thus gradually reducing the overall error.\n",
        "\n",
        "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "Gradient Boosting builds an ensemble of weak learners by iteratively training each new model to predict the residual errors of the combined ensemble of previous models. The predictions of each new model are then scaled by a learning rate and added to the existing ensemble predictions.\n",
        "\n",
        "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
        "1. **Initialize the model** with a constant value, typically the mean of the target variable for regression.\n",
        "2. **Iteratively add models** to the ensemble:\n",
        "   - Compute the negative gradient of the loss function with respect to the current predictions (these are the pseudo-residuals).\n",
        "   - Fit a weak learner (e.g., a decision tree) to the pseudo-residuals.\n",
        "   - Scale the predictions of the weak learner by a learning rate.\n",
        "   - Update the ensemble predictions by adding the scaled predictions of the new weak learner.\n",
        "3. **Combine the models** by summing their scaled predictions to form the final model.\n",
        "\n",
        "Here's a more detailed step-by-step for a simple Gradient Boosting algorithm:\n",
        "\n",
        "1. **Initialization:** Start with an initial prediction, typically the mean of the target values.\n",
        "   \\[ F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^{n} L(y_i, \\gamma) \\]\n",
        "   For mean squared error, this is simply the mean of \\( y \\).\n",
        "\n",
        "2. **For each iteration \\( m = 1 \\) to \\( M \\):**\n",
        "   - Compute the pseudo-residuals (negative gradient of the loss function):\n",
        "     \\[ r_i^{(m)} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x_i) = F_{m-1}(x_i)} \\]\n",
        "   - Fit a weak learner (e.g., decision tree) to the pseudo-residuals:\n",
        "     \\[ h_m(x) = \\arg\\min_h \\sum_{i=1}^{n} \\left(r_i^{(m)} - h(x_i)\\right)^2 \\]\n",
        "   - Update the model with the learning rate \\( \\eta \\):\n",
        "     \\[ F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x) \\]\n",
        "\n",
        "3. **Output the final model**:\n",
        "   \\[ F(x) = F_M(x) \\]\n",
        "\n",
        "By iteratively fitting weak learners to the residuals of the previous model, Gradient Boosting effectively reduces the overall error and improves predictive performance."
      ],
      "metadata": {
        "id": "nSn8zP2YgOwG"
      }
    }
  ]
}