{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb6N7lJ1EuOZmYiDLvVdQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/20april.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t9JjvkZHTay"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is the KNN algorithm?\n",
        "\n",
        "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and lazy learning algorithm used for classification and regression tasks. It works by finding the 'k' closest data points (neighbors) to a given input point and making predictions based on the majority class (for classification) or the average value (for regression) of these neighbors.\n",
        "\n",
        "### Q2. How do you choose the value of K in KNN?\n",
        "\n",
        "Choosing the value of 'K' is crucial for the performance of the KNN algorithm. Here are some common methods:\n",
        "\n",
        "1. **Cross-validation**: Use cross-validation to test different values of 'K' and select the one that provides the best performance on the validation set.\n",
        "2. **Rule of thumb**: A common starting point is \\( K = \\sqrt{N} \\), where \\( N \\) is the number of data points in the training set.\n",
        "3. **Odd number**: For binary classification, an odd value of 'K' is often chosen to avoid ties.\n",
        "4. **Experimentation**: Sometimes, experimentation with different values of 'K' is necessary to find the optimal value.\n",
        "\n",
        "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "- **KNN Classifier**: Used for classification tasks. It predicts the class of a data point based on the majority class among its 'k' nearest neighbors.\n",
        "- **KNN Regressor**: Used for regression tasks. It predicts the value of a data point based on the average (or weighted average) value of its 'k' nearest neighbors.\n",
        "\n",
        "### Q4. How do you measure the performance of KNN?\n",
        "\n",
        "The performance of KNN can be measured using various metrics depending on whether it's a classification or regression task:\n",
        "\n",
        "- **Classification**:\n",
        "  - Accuracy\n",
        "  - Precision\n",
        "  - Recall\n",
        "  - F1-score\n",
        "  - ROC-AUC\n",
        "\n",
        "- **Regression**:\n",
        "  - Mean Absolute Error (MAE)\n",
        "  - Mean Squared Error (MSE)\n",
        "  - Root Mean Squared Error (RMSE)\n",
        "  - R-squared (coefficient of determination)\n",
        "\n",
        "### Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "The curse of dimensionality refers to the phenomenon where the performance of the KNN algorithm degrades as the number of features (dimensions) increases. In high-dimensional spaces, the distance between data points becomes less meaningful, and the algorithm may struggle to find close neighbors, leading to poor performance.\n",
        "\n",
        "### Q6. How do you handle missing values in KNN?\n",
        "\n",
        "There are several strategies to handle missing values in KNN:\n",
        "\n",
        "1. **Imputation**: Replace missing values with the mean, median, or mode of the feature.\n",
        "2. **Remove missing values**: If the number of missing values is small, remove the instances with missing values.\n",
        "3. **Use a model**: Train a model to predict missing values based on other features.\n",
        "\n",
        "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "\n",
        "- **KNN Classifier**:\n",
        "  - Better for problems where class labels are categorical.\n",
        "  - Performance can be affected by class imbalance.\n",
        "  - Works well for small to medium-sized datasets with well-separated classes.\n",
        "\n",
        "- **KNN Regressor**:\n",
        "  - Better for problems where the target variable is continuous.\n",
        "  - Sensitive to outliers since it averages the values of neighbors.\n",
        "  - Works well when the relationship between features and target variable is smooth and local.\n",
        "\n",
        "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
        "\n",
        "- **Strengths**:\n",
        "  - Simple to implement and understand.\n",
        "  - No training phase; all computation is deferred until prediction.\n",
        "  - Can work well with a reasonable number of features and training examples.\n",
        "\n",
        "- **Weaknesses**:\n",
        "  - Computationally expensive at prediction time due to the need to compute distances between the input point and all points in the training set.\n",
        "  - Performance degrades with high-dimensional data (curse of dimensionality).\n",
        "  - Sensitive to irrelevant or redundant features.\n",
        "  - Sensitive to the choice of distance metric and value of 'K'.\n",
        "\n",
        "- **Addressing Weaknesses**:\n",
        "  - Use feature selection or dimensionality reduction techniques like PCA.\n",
        "  - Scale features to ensure equal contribution to distance calculations.\n",
        "  - Experiment with different distance metrics and values of 'K'.\n",
        "  - Use efficient data structures like KD-Trees or Ball Trees for faster neighbor search.\n",
        "\n",
        "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "\n",
        "- **Euclidean Distance**: The straight-line distance between two points in Euclidean space. It is calculated as:\n",
        "  \\[\n",
        "  d(p, q) = \\sqrt{\\sum_{i=1}^n (p_i - q_i)^2}\n",
        "  \\]\n",
        "  where \\( p \\) and \\( q \\) are two points in n-dimensional space.\n",
        "\n",
        "- **Manhattan Distance**: The sum of the absolute differences between the coordinates of two points. It is calculated as:\n",
        "  \\[\n",
        "  d(p, q) = \\sum_{i=1}^n |p_i - q_i|\n",
        "  \\]\n",
        "  Euclidean distance is more sensitive to large differences in any one dimension, while Manhattan distance treats all dimensions equally.\n",
        "\n",
        "### Q10. What is the role of feature scaling in KNN?\n",
        "\n",
        "Feature scaling is crucial in KNN because the algorithm relies on distance calculations. Features with larger ranges can disproportionately influence the distance metric, leading to biased predictions. Common scaling methods include:\n",
        "\n",
        "- **Standardization**: Transforming features to have zero mean and unit variance.\n",
        "- **Normalization**: Scaling features to a fixed range, usually [0, 1].\n",
        "\n",
        "Scaling ensures that all features contribute equally to the distance calculations, leading to more accurate and reliable predictions."
      ],
      "metadata": {
        "id": "c866Uij7JsiH"
      }
    }
  ]
}