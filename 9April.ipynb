{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLoYya5JnZXAqC1nUxykm8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/9April.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkwRDsCuai6g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Bayes' theorem?\n",
        "Bayes' theorem is a fundamental theorem in probability theory that describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It's a way to update the probability of a hypothesis as more evidence or information becomes available.\n",
        "\n",
        "### Q2. What is the formula for Bayes' theorem?\n",
        "The formula for Bayes' theorem is:\n",
        "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
        "where:\n",
        "- \\( P(A|B) \\) is the posterior probability: the probability of event \\( A \\) occurring given that \\( B \\) is true.\n",
        "- \\( P(B|A) \\) is the likelihood: the probability of event \\( B \\) occurring given that \\( A \\) is true.\n",
        "- \\( P(A) \\) is the prior probability: the initial probability of event \\( A \\).\n",
        "- \\( P(B) \\) is the marginal probability: the total probability of event \\( B \\).\n",
        "\n",
        "### Q3. How is Bayes' theorem used in practice?\n",
        "Bayes' theorem is used in various fields for different purposes, such as:\n",
        "- **Medical Diagnosis**: To update the probability of a disease given a test result.\n",
        "- **Spam Filtering**: To determine the probability that an email is spam given the presence of certain words.\n",
        "- **Machine Learning**: In algorithms like Naive Bayes classifiers, where it helps in classifying data points.\n",
        "- **Finance**: To update the probability of market events based on new information.\n",
        "\n",
        "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
        "Bayes' theorem is a direct application of conditional probability. It provides a mathematical way to reverse conditional probabilities. Specifically, it allows us to compute the conditional probability of \\( A \\) given \\( B \\) when we know the conditional probability of \\( B \\) given \\( A \\), along with the individual probabilities of \\( A \\) and \\( B \\).\n",
        "\n",
        "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
        "The choice of Naive Bayes classifier depends on the nature of the input data:\n",
        "- **Gaussian Naive Bayes**: Used when the features are continuous and normally distributed.\n",
        "- **Multinomial Naive Bayes**: Suitable for discrete data like word counts or frequency data in text classification problems.\n",
        "- **Bernoulli Naive Bayes**: Appropriate for binary/boolean features where features are binary (0 or 1).\n",
        "\n",
        "### Q6. Assignment: Naive Bayes Classification\n",
        "\n",
        "Given the dataset with two features, \\( X1 \\) and \\( X2 \\), and two classes, \\( A \\) and \\( B \\), we need to classify a new instance with features \\( X1 = 3 \\) and \\( X2 = 4 \\). Assuming equal prior probabilities for each class, we can calculate the posterior probabilities using Naive Bayes.\n",
        "\n",
        "**Frequency Table:**\n",
        "| Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
        "|-------|------|------|------|------|------|------|------|\n",
        "| A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
        "| B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n",
        "\n",
        "**Calculations:**\n",
        "1. **Calculate Prior Probabilities:**\n",
        "   \\[\n",
        "   P(A) = P(B) = 0.5\n",
        "   \\]\n",
        "\n",
        "2. **Calculate Likelihoods:**\n",
        "   \\[\n",
        "   P(X1=3 | A) = \\frac{4}{3+3+4} = \\frac{4}{10} = 0.4\n",
        "   \\]\n",
        "   \\[\n",
        "   P(X2=4 | A) = \\frac{3}{4+3+3+3} = \\frac{3}{13} \\approx 0.231\n",
        "   \\]\n",
        "   \\[\n",
        "   P(X1=3 | B) = \\frac{1}{2+2+1} = \\frac{1}{5} = 0.2\n",
        "   \\]\n",
        "   \\[\n",
        "   P(X2=4 | B) = \\frac{3}{2+2+2+3} = \\frac{3}{9} = 0.333\n",
        "   \\]\n",
        "\n",
        "3. **Calculate Posteriors:**\n",
        "   \\[\n",
        "   P(A | X1=3, X2=4) = P(X1=3 | A) \\cdot P(X2=4 | A) \\cdot P(A) = 0.4 \\cdot 0.231 \\cdot 0.5 \\approx 0.0462\n",
        "   \\]\n",
        "   \\[\n",
        "   P(B | X1=3, X2=4) = P(X1=3 | B) \\cdot P(X2=4 | B) \\cdot P(B) = 0.2 \\cdot 0.333 \\cdot 0.5 \\approx 0.0333\n",
        "   \\]\n",
        "\n",
        "**Conclusion:**\n",
        "Since \\( P(A | X1=3, X2=4) > P(B | X1=3, X2=4) \\), the Naive Bayes classifier would predict the new instance to belong to class \\( A \\)."
      ],
      "metadata": {
        "id": "0pc7BFj0ajyN"
      }
    }
  ]
}