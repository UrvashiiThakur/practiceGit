{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmyHHqO45/Lt28q2t3nPwY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/29_Mar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpeaghY6vlzy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
        "\n",
        "**Lasso Regression (Least Absolute Shrinkage and Selection Operator)**:\n",
        "- **Concept**: Lasso Regression is a type of linear regression that includes a regularization term to prevent overfitting. It adds a penalty equal to the absolute value of the magnitude of the coefficients to the loss function.\n",
        "- **Equation**:\n",
        "  \\[\n",
        "  \\min_{\\beta} \\left( \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^p |\\beta_j| \\right)\n",
        "  \\]\n",
        "  where \\(\\lambda\\) is the regularization parameter.\n",
        "- **Difference from Other Regression Techniques**:\n",
        "  - **Ordinary Least Squares (OLS)**: Minimizes only the sum of squared residuals without any penalty term, which can lead to overfitting if the model is too complex.\n",
        "  - **Ridge Regression**: Adds a penalty equal to the square of the magnitude of coefficients (L2 regularization). It shrinks coefficients but does not set any to zero.\n",
        "  - **Lasso Regression**: Adds a penalty equal to the absolute value of the magnitude of coefficients (L1 regularization). It can shrink some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
        "\n",
        "The main advantage of using Lasso Regression in feature selection is its ability to shrink some coefficients to exactly zero. This characteristic allows Lasso to effectively reduce the number of predictors in the model, thus performing both variable selection and regularization simultaneously. This is particularly useful when dealing with high-dimensional data with many features, as it can simplify the model and improve interpretability.\n",
        "\n",
        "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
        "\n",
        "Interpreting the coefficients of a Lasso Regression model involves the following:\n",
        "- **Non-Zero Coefficients**: These represent the features that are considered important by the model. The sign and magnitude of the coefficient indicate the direction and strength of the relationship between the feature and the target variable.\n",
        "- **Zero Coefficients**: Features with coefficients shrunk to zero are deemed unimportant by the model and effectively excluded from the prediction equation. This feature selection helps in simplifying the model.\n",
        "\n",
        "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
        "\n",
        "The primary tuning parameter in Lasso Regression is the regularization parameter \\(\\lambda\\):\n",
        "- **\\(\\lambda\\)**: Controls the strength of the L1 penalty. A larger \\(\\lambda\\) increases the amount of shrinkage, pushing more coefficients to zero, leading to a sparser model. Conversely, a smaller \\(\\lambda\\) results in less shrinkage, retaining more features in the model.\n",
        "  - **High \\(\\lambda\\)**: More regularization, fewer features, possibly underfitting.\n",
        "  - **Low \\(\\lambda\\)**: Less regularization, more features, potentially overfitting.\n",
        "\n",
        "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
        "\n",
        "Yes, Lasso Regression can be used for non-linear regression problems through a process called **feature engineering**:\n",
        "- **Polynomial Features**: Transform the original features into polynomial features (e.g., \\(x^2\\), \\(x^3\\), etc.).\n",
        "- **Interaction Terms**: Include interaction terms between features to capture non-linear relationships.\n",
        "- **Implementation**: Apply Lasso Regression to the transformed features. This allows the model to learn non-linear relationships while still benefiting from Lasso's regularization and feature selection capabilities.\n",
        "\n",
        "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
        "\n",
        "**Difference between Ridge Regression and Lasso Regression**:\n",
        "- **Penalty Term**:\n",
        "  - **Ridge Regression**: Uses L2 penalty (\\(\\sum_{j=1}^p \\beta_j^2\\)). It shrinks coefficients but does not set any to zero.\n",
        "  - **Lasso Regression**: Uses L1 penalty (\\(\\sum_{j=1}^p |\\beta_j|\\)). It can shrink some coefficients to zero, performing feature selection.\n",
        "- **Feature Selection**:\n",
        "  - **Ridge Regression**: Retains all features, but shrinks their coefficients.\n",
        "  - **Lasso Regression**: Can exclude irrelevant features by setting their coefficients to zero.\n",
        "- **Multicollinearity**:\n",
        "  - **Ridge Regression**: Better suited for handling multicollinearity among predictors.\n",
        "  - **Lasso Regression**: Can handle multicollinearity to some extent but may arbitrarily select one among highly correlated predictors.\n",
        "\n",
        "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "\n",
        "Yes, Lasso Regression can handle multicollinearity by shrinking some of the correlated features' coefficients to zero, effectively removing them from the model. However, it may arbitrarily select one feature among a set of highly correlated features, potentially leading to instability in the selection process. For severe multicollinearity, Ridge Regression or Elastic Net (which combines L1 and L2 penalties) may be more appropriate.\n",
        "\n",
        "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
        "\n",
        "Choosing the optimal value of \\(\\lambda\\) can be done using the following methods:\n",
        "- **Cross-Validation**: Perform k-fold cross-validation to evaluate the model performance for different values of \\(\\lambda\\) and select the one that minimizes the cross-validated error.\n",
        "- **Grid Search**: Use a grid search over a range of \\(\\lambda\\) values to find the best performing parameter.\n",
        "- **Regularization Path**: Plot the regularization path, which shows the coefficients as a function of \\(\\lambda\\), and select \\(\\lambda\\) based on the desired level of sparsity and model performance.\n",
        "\n",
        "In practice, cross-validation is the most commonly used method for selecting \\(\\lambda\\), as it provides a balance between bias and variance and ensures that the chosen parameter generalizes well to unseen data."
      ],
      "metadata": {
        "id": "6xgCmO1-vpRK"
      }
    }
  ]
}