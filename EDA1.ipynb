{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS+pr/7ekYDSo7rmtxAa0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/EDA1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dLJX4rXwWdM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in predicting the quality of wine.\n",
        "\n",
        "The wine quality data set typically includes the following features:\n",
        "\n",
        "1. **Fixed Acidity**: This refers to acids that do not evaporate easily. It affects the taste and preservation of wine.\n",
        "2. **Volatile Acidity**: This measures the amount of acetic acid in wine, which can lead to an unpleasant vinegar taste if too high.\n",
        "3. **Citric Acid**: Adds freshness and flavor to wine. Small amounts can be beneficial.\n",
        "4. **Residual Sugar**: The amount of sugar left after fermentation. It affects sweetness and can indicate fermentation problems if too high.\n",
        "5. **Chlorides**: The amount of salt in the wine, which can affect its taste.\n",
        "6. **Free Sulfur Dioxide**: Protects wine from oxidation and spoilage.\n",
        "7. **Total Sulfur Dioxide**: The total amount of SO2 present, affecting preservation and taste.\n",
        "8. **Density**: Related to the alcohol and sugar content. Affects mouthfeel and body of the wine.\n",
        "9. **pH**: Measures the acidity or basicity. It affects the taste, stability, and color of the wine.\n",
        "10. **Sulphates**: Adds to the preservation and can enhance flavors.\n",
        "11. **Alcohol**: Higher alcohol content generally improves wine quality, affecting taste and preservation.\n",
        "\n",
        "Each feature is important as it contributes to the overall sensory characteristics of the wine, such as taste, smell, and mouthfeel, which ultimately determine its quality.\n",
        "\n",
        "### Q2. How did you handle missing data in the wine quality data set during the feature engineering process? Discuss the advantages and disadvantages of different imputation techniques.\n",
        "\n",
        "**Handling Missing Data**:\n",
        "- **Removing Rows/Columns**: If missing data is minimal, removing affected rows or columns is straightforward but may lead to data loss.\n",
        "- **Mean/Median Imputation**: Replaces missing values with the mean or median of the column. It’s simple and maintains the sample size but can distort data distribution if missing values are not random.\n",
        "- **Mode Imputation**: Replaces missing values with the mode. Useful for categorical data but less effective for continuous data.\n",
        "- **K-Nearest Neighbors (KNN) Imputation**: Uses the mean of the k-nearest neighbors to impute missing values. It considers data distribution but is computationally intensive.\n",
        "- **Multiple Imputation**: Generates multiple datasets by imputing values based on other features and combines the results. It’s robust but complex and computationally expensive.\n",
        "\n",
        "Each technique has its advantages and disadvantages. For example, mean imputation is quick and easy but can introduce bias, while multiple imputation provides a more accurate estimation but is resource-intensive.\n",
        "\n",
        "### Q3. What are the key factors that affect students' performance in exams? How would you go about analyzing these factors using statistical techniques?\n",
        "\n",
        "**Key Factors**:\n",
        "- **Socioeconomic Status**: Family income, parental education, and occupation.\n",
        "- **Study Habits**: Hours spent studying, consistency, and study environment.\n",
        "- **School Environment**: Teacher quality, classroom size, and school resources.\n",
        "- **Personal Factors**: Motivation, health, sleep, and mental well-being.\n",
        "\n",
        "**Analysis**:\n",
        "- **Descriptive Statistics**: Summarize data using means, medians, and standard deviations.\n",
        "- **Correlation Analysis**: Examine relationships between factors using Pearson or Spearman correlation coefficients.\n",
        "- **Regression Analysis**: Use multiple regression to identify the impact of each factor on performance.\n",
        "- **ANOVA**: Compare means across different groups (e.g., socioeconomic levels).\n",
        "- **Factor Analysis**: Identify underlying variables that explain the data structure.\n",
        "- **Machine Learning**: Use decision trees, random forests, or neural networks for predictive modeling.\n",
        "\n",
        "### Q4. Describe the process of feature engineering in the context of the student performance data set. How did you select and transform the variables for your model?\n",
        "\n",
        "**Feature Engineering Steps**:\n",
        "1. **Data Cleaning**: Handle missing values, correct errors, and remove outliers.\n",
        "2. **Feature Selection**: Identify relevant features using correlation analysis and domain knowledge.\n",
        "3. **Transformation**: Normalize or standardize features, create interaction terms, and encode categorical variables.\n",
        "4. **Derived Features**: Create new features from existing ones, such as average study time per week.\n",
        "5. **Dimensionality Reduction**: Apply techniques like PCA to reduce the number of features while retaining important information.\n",
        "\n",
        "**Selection and Transformation**:\n",
        "- Select features with high correlation to the target variable.\n",
        "- Normalize continuous variables to have a mean of 0 and standard deviation of 1.\n",
        "- Encode categorical variables using one-hot encoding.\n",
        "- Create features like attendance rate, total study hours, and participation in extracurricular activities.\n",
        "- Use PCA to reduce feature dimensionality if necessary.\n",
        "\n",
        "### Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to these features to improve normality?\n",
        "\n",
        "**EDA Steps**:\n",
        "1. **Loading Data**: Use pandas to load the dataset.\n",
        "   ```python\n",
        "   import pandas as pd\n",
        "   wine_data = pd.read_csv('winequality.csv')\n",
        "   ```\n",
        "2. **Summary Statistics**: Generate summary statistics using `describe()`.\n",
        "   ```python\n",
        "   wine_data.describe()\n",
        "   ```\n",
        "3. **Visualize Distributions**: Use histograms and box plots.\n",
        "   ```python\n",
        "   import matplotlib.pyplot as plt\n",
        "   wine_data.hist(bins=20, figsize=(14,10))\n",
        "   plt.show()\n",
        "   ```\n",
        "\n",
        "**Non-normal Features**:\n",
        "- **Residual Sugar**, **Chlorides**, and **Sulphates** often exhibit skewness.\n",
        "\n",
        "**Transformations**:\n",
        "- **Log Transformation**: Applies to skewed data to normalize distribution.\n",
        "  ```python\n",
        "  wine_data['log_residual_sugar'] = np.log(wine_data['residual_sugar'] + 1)\n",
        "  ```\n",
        "- **Box-Cox Transformation**: Another method for normalizing data.\n",
        "  ```python\n",
        "  from scipy import stats\n",
        "  wine_data['boxcox_residual_sugar'], _ = stats.boxcox(wine_data['residual_sugar'] + 1)\n",
        "  ```\n",
        "- **Standardization**: Transform data to have zero mean and unit variance.\n",
        "  ```python\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  wine_data_scaled = scaler.fit_transform(wine_data)\n",
        "  ```\n",
        "\n",
        "### Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of features. What is the minimum number of principal components required to explain 90% of the variance in the data?\n",
        "\n",
        "**Performing PCA**:\n",
        "1. **Standardize the Data**:\n",
        "   ```python\n",
        "   from sklearn.preprocessing import StandardScaler\n",
        "   scaler = StandardScaler()\n",
        "   wine_data_scaled = scaler.fit_transform(wine_data.drop(columns=['quality']))\n",
        "   ```\n",
        "\n",
        "2. **Apply PCA**:\n",
        "   ```python\n",
        "   from sklearn.decomposition import PCA\n",
        "   pca = PCA()\n",
        "   wine_pca = pca.fit_transform(wine_data_scaled)\n",
        "   ```\n",
        "\n",
        "3. **Variance Explained**:\n",
        "   ```python\n",
        "   explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "   ```\n",
        "\n",
        "4. **Plot Variance**:\n",
        "   ```python\n",
        "   plt.plot(explained_variance)\n",
        "   plt.xlabel('Number of Principal Components')\n",
        "   plt.ylabel('Cumulative Explained Variance')\n",
        "   plt.show()\n",
        "   ```\n",
        "\n",
        "5. **Determine Components for 90% Variance**:\n",
        "   - Find the number of components required to reach 90% cumulative explained variance.\n",
        "     ```python\n",
        "     n_components = np.argmax(explained_variance >= 0.90) + 1\n",
        "     ```\n",
        "\n",
        "**Result**:\n",
        "- The minimum number of principal components required to explain 90% of the variance is determined by examining the cumulative explained variance plot and identifying the point where it exceeds 90%.\n",
        "\n",
        "This process helps reduce the feature space while retaining most of the information, making the model more efficient and less prone to overfitting."
      ],
      "metadata": {
        "id": "cwyFBZ1fwZ1n"
      }
    }
  ]
}