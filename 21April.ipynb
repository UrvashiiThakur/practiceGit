{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5l1PzvnszQKWt83LDxRCv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UrvashiiThakur/practiceGit/blob/main/21April.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHdzQARwMy9M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
        "\n",
        "**Euclidean Distance**: Measures the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between corresponding coordinates.\n",
        "\n",
        "\\[ d_{\\text{Euclidean}}(x, y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} \\]\n",
        "\n",
        "**Manhattan Distance**: Measures the distance between two points along axes at right angles. It is calculated as the sum of the absolute differences between corresponding coordinates.\n",
        "\n",
        "\\[ d_{\\text{Manhattan}}(x, y) = \\sum_{i=1}^n |x_i - y_i| \\]\n",
        "\n",
        "**Effect on Performance**:\n",
        "- Euclidean distance is sensitive to outliers because it squares the differences.\n",
        "- Manhattan distance can be more robust in high-dimensional spaces and less sensitive to outliers.\n",
        "- The choice of distance metric can affect the shape of the decision boundary. Euclidean tends to form circular boundaries, while Manhattan forms square boundaries.\n",
        "\n",
        "### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
        "\n",
        "**Choosing Optimal k**:\n",
        "- **Cross-Validation**: Use k-fold cross-validation to evaluate the performance of different k values. Select the k that gives the best performance on validation sets.\n",
        "- **Grid Search**: Perform a grid search over a range of k values and evaluate the performance using cross-validation.\n",
        "- **Elbow Method**: Plot the error rate or another performance metric against different k values. The point where the error starts to decrease more slowly (the \"elbow\") can be a good choice for k.\n",
        "\n",
        "### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
        "\n",
        "**Choice of Distance Metric**:\n",
        "- **Euclidean Distance**: Suitable when the features have similar scales and the data points are close to each other. Used in scenarios where the distance between points should reflect straight-line proximity.\n",
        "- **Manhattan Distance**: Suitable for high-dimensional spaces or when features have different scales. Used in grid-like path problems (e.g., urban street planning).\n",
        "- **Situational Choice**: Choose Euclidean for well-scaled data with low dimensions. Choose Manhattan for high-dimensional data or when robustness to outliers is needed.\n",
        "\n",
        "### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
        "\n",
        "**Common Hyperparameters**:\n",
        "- **k (Number of Neighbors)**: Affects bias-variance tradeoff. Low k increases variance (overfitting), high k increases bias (underfitting).\n",
        "- **Distance Metric**: Affects how distances are calculated and the shape of the decision boundary.\n",
        "- **Weighting Function**: Uniform or distance-based weights. Distance-based can give more importance to closer neighbors.\n",
        "\n",
        "**Tuning Hyperparameters**:\n",
        "- **Grid Search**: Explore combinations of hyperparameters using cross-validation.\n",
        "- **Random Search**: Randomly sample hyperparameters and evaluate performance.\n",
        "- **Cross-Validation**: Use k-fold cross-validation to assess the performance of different hyperparameter settings.\n",
        "\n",
        "### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
        "\n",
        "**Effect of Training Set Size**:\n",
        "- **Large Training Set**: Improves model performance by providing more data for the model to learn from, but increases computational cost.\n",
        "- **Small Training Set**: May lead to overfitting or underfitting and poor generalization.\n",
        "\n",
        "**Optimizing Training Set Size**:\n",
        "- **Subset Selection**: Use a representative subset of the training data if the dataset is too large.\n",
        "- **Data Augmentation**: Generate more data to increase the size of the training set.\n",
        "- **Feature Selection**: Reduce the number of features to improve computational efficiency without compromising performance.\n",
        "\n",
        "### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
        "\n",
        "**Drawbacks**:\n",
        "- **Computationally Expensive**: KNN requires storing the entire training dataset and computing distances for each prediction, which can be slow for large datasets.\n",
        "- **Sensitive to Irrelevant Features**: KNN performance can degrade if there are many irrelevant or redundant features.\n",
        "- **Curse of Dimensionality**: In high-dimensional spaces, the distance between points becomes less meaningful.\n",
        "\n",
        "**Overcoming Drawbacks**:\n",
        "- **Dimensionality Reduction**: Use PCA, LDA, or other techniques to reduce the number of features.\n",
        "- **Feature Scaling**: Standardize or normalize features to ensure they contribute equally to the distance metric.\n",
        "- **Efficient Algorithms**: Use approximate nearest neighbor algorithms or KD-Trees to speed up distance computations.\n",
        "- **Feature Selection**: Select the most relevant features to improve performance and reduce computational cost."
      ],
      "metadata": {
        "id": "ep88y-42M0KY"
      }
    }
  ]
}